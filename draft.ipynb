{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chat_classification_TFS_ML(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "CT6TbvERoIJd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import razdel\n",
        "import maru\n",
        "import zipfile\n",
        "import warnings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from numpy import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "719BQ_9jWh3w",
        "colab_type": "code",
        "outputId": "f09cbf7c-7883-4f68-a1de-d5c5d62899bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1281
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install razdel\n",
        "! pip install maru\n",
        "! pip install xgboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/94/c2/742bc726aad693c964b051481f0cb71937556885e25201fab1c7f8fae0b8/razdel-0.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.3.0\n",
            "Collecting maru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/1f/7a48ecead8d80622bd1be9974ce5600d3af4e21ebda7f6d1004649c3847e/maru-0.1.1-py3-none-any.whl (44.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 44.3MB 1.1MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.0 (from maru)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/29/f4c845648ed23264e986cdc5fbab5f8eace1be5e62144ef69ccc7189461d/numpy-1.15.0-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from maru) (0.20.2)\n",
            "Collecting python-crfsuite>=0.9.5 (from maru)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K    100% |████████████████████████████████| 757kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from maru) (1.1.0)\n",
            "Collecting lru-dict>=1.1.6 (from maru)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
            "Requirement already satisfied: keras>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from maru) (2.2.4)\n",
            "Collecting pymorphy2[fast]==0.8 (from maru)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from maru) (1.13.0rc1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.2->maru) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.2->maru) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.2->maru) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.2->maru) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.2->maru) (1.0.9)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]==0.8->maru) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2[fast]==0.8->maru)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.1MB 6.9MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7 (from pymorphy2[fast]==0.8->maru)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting DAWG>=0.7.3; extra == \"fast\" (from pymorphy2[fast]==0.8->maru)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c0/d8d967bcaa0b572f9dc1d878bbf5a7bfd5afa2102a5ae426731f6ce3bc26/DAWG-0.7.8.tar.gz (255kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (1.12.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (0.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (1.13.0rc0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (0.33.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.9.0->maru) (3.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.9.0->maru) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.9.0->maru) (3.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow>=1.9.0->maru) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.9.0->maru) (40.8.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow>=1.9.0->maru) (5.1.2)\n",
            "Building wheels for collected packages: lru-dict, DAWG\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d4/88/d0/4e4abc83eb8f59a71e8dbd8ba99fd5615a3af1fac1ef7f8825\n",
            "Successfully built lru-dict DAWG\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, python-crfsuite, lru-dict, pymorphy2-dicts, dawg-python, DAWG, pymorphy2, maru\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed DAWG-0.7.8 dawg-python-0.7.2 lru-dict-1.1.6 maru-0.1.1 numpy-1.15.0 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 python-crfsuite-0.9.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.7.post4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yahjQ_f_v8ld",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gJCMxh6cr7bu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data, test = pd.read_csv(\"train.csv\"), pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FL9b0nY4R4n4"
      },
      "cell_type": "markdown",
      "source": [
        "ТРАНСФОРМАЦИЯ"
      ]
    },
    {
      "metadata": {
        "id": "zBjkzwGYBw3T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Дальше идут вспомогательные функции к главной - transform, которая преобразует исходный сырой текст в нужный формат (удаление лишних символов, нормализация и тд, там в функции все написано)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yzC9dNymSX8c",
        "outputId": "d5f27c89-045b-42a3-e1d7-52ff898fc296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = set(stopwords.words('russian'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_OF_ZPblR9D3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def makeCorpus(vectors):\n",
        "  corpus = []\n",
        "  for vector in vectors:\n",
        "    corpus.append(' '.join(vector))\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z8HqB-17SCY4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def isShortnumber(number):\n",
        "   return True if len(number)<=4 and number.isdigit() else False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MEevBjCTSE_a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(text, stopWords):\n",
        "    vectors = []\n",
        "    a = re.compile(r'(?<=[А-Яа-я0-9])\\.(?=[А-Яа-я])')\n",
        "    b = re.compile(r'(?<=[А-Яа-я])\\.(?=[А-Яа-я0-9])')\n",
        "    c = re.compile(r'(?<=[А-Яа-я])\\.(?=[A-Za-z])')\n",
        "    d = re.compile(r'(?<=[A-Za-z])\\.(?=[А-Яа-я])')\n",
        "    for line in text:\n",
        "        line = a.sub(' ', line)\n",
        "        line = b.sub(' ', line)\n",
        "        line = c.sub(' ', line)\n",
        "        line = d.sub(' ', line)\n",
        "        vectors.append([i.strip('. -') for i in line.split() if i.strip('. -') not in stopWords and not isShortnumber(i.strip('. -'))\\\n",
        "                      and len(i.strip('. -')) > 1])\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p8vWXyA3SH02",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maru_lematize(text, analyzer):\n",
        "  res_lem = []\n",
        "  for line in text:\n",
        "    res_lem.append([word.lemma for word in analyzer.analyze(line)])\n",
        "  return res_lem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YCDGp-nuSKdZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform(text):\n",
        "  vectors = []\n",
        "  \n",
        "  #########lowercase#################\n",
        "  text = text.apply(lambda x: x.lower())\n",
        "  \n",
        "  \n",
        "  ###########delete \\n################\n",
        "  n = re.compile(r'\\\\n')\n",
        "  text = text.apply(lambda x: n.sub(' ', x))\n",
        "  \n",
        "  \n",
        "  ##############delete everything apart ё, -, \\.##############\n",
        "  pattern_mod = re.compile(r'[^a-zA-Z0-9а-яА-я ё.\\-]')\n",
        "  text = text.apply(lambda x: pattern_mod.sub(' ', x))\n",
        "  \n",
        "  \n",
        "  ################carefully deleting \\-#####################\n",
        "  pattern_def = re.compile(r'(?<![A-Za-zА-Яа-я])\\-(?![A-Za-zА-Яа-я])')\n",
        "  text = text.apply(lambda x: pattern_def.sub(' ', x))\n",
        "  \n",
        "  \n",
        "  #################delete short numbers#####################\n",
        "  short_numbers = re.compile(r'(?<![0-9])(?<!\\.)[0-9]{1,4}(?![0-9])(?!\\.)')\n",
        "  text = text.apply(lambda x: short_numbers.sub(r'', x))\n",
        "  \n",
        "  #################delete stopwords###################\n",
        "  stopWords = set(stopwords.words('russian'))\n",
        "  for line in text.index:\n",
        "    text[line] = ' '.join([elem for elem in text[line].split() if elem not in stopWords])\n",
        "    \n",
        "    \n",
        "  ###################tokenize######################\n",
        "  text = tokenize(text, stopWords)\n",
        "  \n",
        "  \n",
        "  ###################normalize######################\n",
        "  analyzer = maru.get_analyzer(tagger='linear')\n",
        "  text = maru_lematize(text, analyzer)\n",
        "  \n",
        "  \n",
        "  ####################make corpus#####################\n",
        "  corpus = makeCorpus(text)\n",
        "  \n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khNXm7OpCOoE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "После того, как преобразовали текст, векторизуем его"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wMLnxHGRSSkf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_tfidf_vectorizer(corpus):\n",
        "  tfidf_vectorizer = TfidfVectorizer(tokenizer = lambda x: x.split())\n",
        "  vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "  return [vectors, tfidf_vectorizer]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQhxamPgWY5L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ПОДГОТОВКА ДАННЫХ"
      ]
    },
    {
      "metadata": {
        "id": "jczC13bZWY5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtBdURK9WY5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, tfidfvectorizer = make_tfidf_vectorizer(transform(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVuHHf6gWY55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_corpus = transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4tSAAoWWY6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test = tfidfvectorizer.transform(X_test_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kwPHKO69nOkS"
      },
      "cell_type": "markdown",
      "source": [
        "МОДЕЛЬ"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4NgMnyYmZQo-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###########simple svc###################\n",
        "model = LinearSVC(multi_class='ovr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h93H3ZYyWY6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############simple svc with balanced classes###############\n",
        "model = LinearSVC(multi_class='ovr', class_weight='balanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f0MB9RlbWY6o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############Gradient Boosting with 1000 base algorithms##############\n",
        "model = GradientBoostingClassifier(n_estimators = 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNhjqkzyWY6u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################Random forest with 500 trees################\n",
        "model = RandomForestClassifier(n_estimators=500, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHHTTWb7WY62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################bagging classifier svc balanced weights################\n",
        "model = BaggingClassifier(base_estimator=LinearSVC(multi_class='ovr'), n_estimators=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tARVMKHqWY6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############bagging sgd classifier######################\n",
        "model = BaggingClassifier(base_estimator=SGDClassifier(), n_estimators=10, max_samples=0.9, max_features=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bet4URFCWY7E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############Boosting with SVC as initial#####################\n",
        "class SVCmod:\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        self.svc = LinearSVC().fit(X, y)\n",
        "        return self.svc\n",
        "    def predict(self, X):\n",
        "        labels = self.svc.predict(X)\n",
        "        one_hot_0 = np.array(labels == 0, dtype=np.float64)\n",
        "        one_hot_1 = np.array(labels == 1, dtype=np.float64)\n",
        "        one_hot_2 = np.array(labels == 2, dtype=np.float64)\n",
        "        y = np.column_stack((one_hot_0, one_hot_1, one_hot_2))\n",
        "        return y\n",
        "one = SVCmod()\n",
        "model = GradientBoostingClassifier(init=one)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1nyP3PpvWY7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################boosting with SVC initial #2 ######################\n",
        "one = SVCmod()\n",
        "model = GradientBoostingClassifier(init=one, n_estimators=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pvNg7cq6oKUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Это была попытка запустить ГБ из начального приближения с LinearSVC, который работает лучше всех. Я обернул класс LinearSVC, чтобы он возвращал предсказания в нужном формате, исходный класс не подходит, чтобы использовать его в бустинге на начальном приближении.  Не сработала идея"
      ]
    },
    {
      "metadata": {
        "id": "NlbrpQKtWY7R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############XGBoost###############\n",
        "params = {'max_depth':[5, 8, 15], 'n_estimators':[50, 70, 150], 'booster':['gbtree', 'gblinear', 'dart']}\n",
        "estimator = XGBClassifier()\n",
        "model = GridSearchCV(estimator, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2DmOQ5X8C4eq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Дальше я пытаюсь найти шумовые объекты, для этого считаю отступы и удаляю те, на которых большой отрицательный отступ"
      ]
    },
    {
      "metadata": {
        "id": "h0LEismJkX9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########### find shum ###################\n",
        "\n",
        "def sigmoid(a):\n",
        "  return [1/(1 + np.exp(-elem)) for elem in a]\n",
        "\n",
        "################ prepare all the data ##################\n",
        "X, tfidf = make_tfidf_vectorizer(transform(data['text']))\n",
        "\n",
        "\n",
        "############## fit linear estimator and make predictions #################\n",
        "find_noise = LinearSVC()\n",
        "find_noise.fit(X, data['label'])\n",
        "labels_train_for_confidence = find_noise.predict(X)\n",
        "\n",
        "\n",
        "################## extract confidence ############# это вероятности\n",
        "probs_for_confidence = list(map(lambda x: sigmoid(x) , find_noise.decision_function(X)))\n",
        "\n",
        "\n",
        "########### find margins ################## отступы нахожу, как вероятность умножить на 1, если класс правильный и на -1 в противном\n",
        "otstups = []\n",
        "\n",
        "for triple in zip(probs_for_confidence, labels_train_for_confidence, data['label']):\n",
        "  otstups.append(max(triple[0]) if triple[1]==triple[2] else -max(triple[0]))\n",
        "  \n",
        "######### plot the distribution of margins ##########3\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.hist(otstups, bins=10, edgecolor='b');\n",
        "\n",
        "############### find noise indexes ################# Здесь еще устанавливаю порог, по которому считаю, что объект - шум\n",
        "\n",
        "threshold = -0.7\n",
        "\n",
        "indexes = []\n",
        "\n",
        "for index in range(len(otstups)):\n",
        "  if otstups[index]<threshold:\n",
        "    indexes.append(index)\n",
        "    \n",
        "############ remove the noise ##############\n",
        "\n",
        "X_train_no_noise = data.drop(indexes)\n",
        "\n",
        "############### train the model on the data without noise ################\n",
        "\n",
        "X_train_nn, tfidfvectorizer_nn = make_tfidf_vectorizer(transform(X_train_no_noise['text']))\n",
        "X_test_nn = tfidfvectorizer_nn.transform(transform(test['text']))\n",
        "\n",
        "######### fit the model ###################\n",
        "\n",
        "\n",
        "model_no_noise = LinearSVC(class_weight='balanced')\n",
        "model_no_noise.fit(X_train_nn, X_train_no_noise['label'])\n",
        "\n",
        "########### make prediction ###############\n",
        "\n",
        "predictions_no_noise = model_no_noise.predict(X_test_nn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TTMIveiskL-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Поиск шума и попытка обучения как бы на данных без шумовых объектов не удалась, хотя шум действительно вроде получилось найти"
      ]
    },
    {
      "metadata": {
        "id": "RPuMXHyNDcLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Изначально получился очень большой словарь и число фичей почти сравнимо с числом объектов, поэтому бустинги могли плохо работать, поэтому я решил найти самые-самые фичи и использовать их для трэйна"
      ]
    },
    {
      "metadata": {
        "id": "3CKdJEjiMIWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############## feature importants ####################3\n",
        "\n",
        "################## fit the gs tree ###################\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "params_first_it = {'max_depth':[i*2 for i in range(1, 15)]}\n",
        "gs_first_it = GridSearchCV(tree, params_first_it)\n",
        "\n",
        "gs_first_it.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "############# know the feature importance ###############\n",
        "importance_first_it = gs_first_it.best_estimator_.feature_importances_\n",
        "\n",
        "############### find influential ################## здесь извлекаются фичи, у которых скор важности не ноль\n",
        "\n",
        "new_features_first_it = [elem[1] for elem in zip(importance_first_it, range(len(importance_first_it))) if elem[0]]\n",
        "\n",
        "############# extract influential data ############# здесь данные преобразуются к набору данных на важных фичах\n",
        "X_train_composed_1_it = X_train[:, new_features_first_it]\n",
        "X_test_composed_1_it = X_test[:, new_features_first_it]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4_NHe4e0EJE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Несмотря на то, что у многих фичей был скор важности ноль, предсказательной способности в оставшихся фичах не достаточно, поэтому  я решил убрать из начального множества фичей самые важные, на оставшихся зафитить, и посчитать скор важности оставшихся, но в отсутсвие самых важных признаков"
      ]
    },
    {
      "metadata": {
        "id": "7cOlpf02hOvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############### do one more iteration ######################\n",
        "\n",
        "################ prepare data ########################\n",
        "X_train_low = X_train[:, [i for i in range(X_train.shape[1]) if i not in new_features_first_it]]\n",
        "X_test_low = X_test[:, [i for i in range(X_train.shape[1]) if i not in new_features_first_it]]\n",
        "\n",
        "\n",
        "############## fit another gs tree ####################\n",
        "tree = DecisionTreeClassifier()\n",
        "params_sec_it = {'max_depth':[i*2 for i in range(1, 20)]}\n",
        "gs_sec_it = GridSearchCV(tree, params_sec_it)\n",
        "\n",
        "gs_sec_it.fit(X_train_low, y_train)\n",
        "\n",
        "############## know the feature importance ###############\n",
        "importance_sec_it = gs_sec_it.best_estimator_.feature_importances_\n",
        "\n",
        "############### find influential ##################\n",
        "\n",
        "new_features_sec_it = [elem[1] for elem in zip(importance_sec_it, range(len(importance_sec_it))) if elem[0]]\n",
        "\n",
        "############# extract influential data #############\n",
        "X_train_composed_2_it = X_train_low[:, new_features_sec_it]\n",
        "X_test_composed_2_it = X_test_low[:, new_features_sec_it]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "asmPcA7tkWBR",
        "colab_type": "code",
        "outputId": "756c8c02-07d3-4152-f9e1-f7811b8c2ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "######### test after two iterations #################\n",
        "from scipy.sparse import hstack\n",
        "X_train_composed = hstack((X_train_composed_1_it, X_train_composed_2_it))\n",
        "X_test_composed = hstack((X_test_composed_1_it, X_test_composed_2_it))\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_composed, y_train)\n",
        "res = clf.predict(X_test_composed)\n",
        "print(classification_report(y_test, res))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.40      0.52      6499\n",
            "           1       0.96      0.91      0.93     14882\n",
            "           2       0.69      0.89      0.77     12360\n",
            "\n",
            "   micro avg       0.80      0.80      0.80     33741\n",
            "   macro avg       0.79      0.73      0.74     33741\n",
            "weighted avg       0.81      0.80      0.80     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f3L5r3TnnFgb",
        "colab_type": "code",
        "outputId": "7e388bfd-133f-4b52-dd15-06033c351c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "########### check another algorithm ####################\n",
        "\n",
        "xgb = XGBClassifier(max_depth=28, n_estimators=500)\n",
        "xgb.fit(X_train_composed, y_train)\n",
        "res_ = xgb.predict(X_test_composed)\n",
        "print(classification_report(y_test, res_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.42      0.52      6499\n",
            "           1       0.96      0.91      0.93     14882\n",
            "           2       0.69      0.87      0.77     12360\n",
            "\n",
            "   micro avg       0.80      0.80      0.80     33741\n",
            "   macro avg       0.77      0.73      0.74     33741\n",
            "weighted avg       0.81      0.80      0.79     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YNFuivM2kqjj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Это была попытка сократить размер данных при помощи поиска важных фичей, направление вроде правильное, то есть уже на 600 фичах вместо 60 тыс достигается 0.8, но это не совсем то, чего ожидалось"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-Phrl0xhTBwv"
      },
      "cell_type": "markdown",
      "source": [
        " ТЕСТИРОВАНИЕ МОДЕЛИ"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gw-8yQROXCy7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fz6sCyCGnYFV",
        "outputId": "08adb360-d556-4f0f-85c1-65e5a0f213fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "cell_type": "code",
      "source": [
        "print('simple linear svc')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simple linear svc\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.69      0.72      6499\n",
            "           1       0.98      0.97      0.97     14882\n",
            "           2       0.82      0.87      0.84     12360\n",
            "\n",
            "   micro avg       0.88      0.88      0.88     33741\n",
            "   macro avg       0.85      0.84      0.85     33741\n",
            "weighted avg       0.88      0.88      0.88     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gs0jF50fWY7w",
        "colab_type": "code",
        "outputId": "00f2fe2e-d60a-4b7a-e587-ff50b17a4e20",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('simple linear svc with balanced weights')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simple linear svc with balanced weights\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.73      0.73      6499\n",
            "           1       0.98      0.97      0.97     14882\n",
            "           2       0.83      0.84      0.84     12360\n",
            "\n",
            "   micro avg       0.87      0.87      0.87     33741\n",
            "   macro avg       0.84      0.85      0.85     33741\n",
            "weighted avg       0.88      0.87      0.88     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cT1jWEnLWY8B",
        "colab_type": "code",
        "outputId": "a2e62f54-c2d6-4ab0-c59a-e91410f7b38d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('gradient boosting with 1000 basics')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient boosting with 1000 basics\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.48      0.60      6499\n",
            "           1       0.97      0.92      0.95     14882\n",
            "           2       0.72      0.92      0.81     12360\n",
            "\n",
            "   micro avg       0.84      0.84      0.84     33741\n",
            "   macro avg       0.84      0.77      0.79     33741\n",
            "weighted avg       0.85      0.84      0.83     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lboXsRUiWY8J",
        "colab_type": "code",
        "outputId": "f090002e-db92-4ff7-fa12-8ed406423d47",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('random forest with 500 trees')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest with 500 trees\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.45      0.57      6499\n",
            "           1       0.96      0.96      0.96     14882\n",
            "           2       0.73      0.90      0.81     12360\n",
            "\n",
            "   micro avg       0.84      0.84      0.84     33741\n",
            "   macro avg       0.82      0.77      0.78     33741\n",
            "weighted avg       0.84      0.84      0.83     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H3kpo7-XWY8N",
        "colab_type": "code",
        "outputId": "d3987cdd-d2c2-452c-8066-b0095ea13d75",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('bagging on 100 svc')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bagging on 100 svc\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.69      0.72      6499\n",
            "           1       0.98      0.97      0.97     14882\n",
            "           2       0.82      0.87      0.84     12360\n",
            "\n",
            "   micro avg       0.88      0.88      0.88     33741\n",
            "   macro avg       0.85      0.84      0.85     33741\n",
            "weighted avg       0.88      0.88      0.88     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "quym_HrlWY8V",
        "colab_type": "code",
        "outputId": "8e5a7094-3a42-4abe-e5a6-e6564f12abe5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('SGD classifier bagging 10')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD classifier bagging\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.45      0.59      6499\n",
            "           1       0.97      0.95      0.96     14882\n",
            "           2       0.73      0.93      0.82     12360\n",
            "\n",
            "   micro avg       0.85      0.85      0.85     33741\n",
            "   macro avg       0.85      0.78      0.79     33741\n",
            "weighted avg       0.86      0.85      0.84     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kKuCh749WY8e",
        "colab_type": "code",
        "outputId": "1b58ff84-47d4-48f3-b5fe-82f65246c80d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('Boosting with initial SVC')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boosting with initial SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.69      0.72      6499\n",
            "           1       0.98      0.97      0.97     14882\n",
            "           2       0.82      0.87      0.84     12360\n",
            "\n",
            "   micro avg       0.88      0.88      0.88     33741\n",
            "   macro avg       0.85      0.84      0.84     33741\n",
            "weighted avg       0.88      0.88      0.88     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wPESLwjTWY8k",
        "colab_type": "code",
        "outputId": "ba549d2f-efd9-4fbc-9626-d4126a64aa48",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print('XGBoost')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.16      0.26      6499\n",
            "           1       0.97      0.82      0.89     14882\n",
            "           2       0.60      0.96      0.74     12360\n",
            "\n",
            "   micro avg       0.75      0.75      0.75     33741\n",
            "   macro avg       0.80      0.65      0.63     33741\n",
            "weighted avg       0.81      0.75      0.71     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w-Mv52EEWY8p",
        "colab_type": "code",
        "outputId": "c3dc7002-4549-41a5-c432-49037efcfe45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "result = model.predict(X_test)\n",
        "print(model.best_estimator_)\n",
        "print('XGBoost with GS')\n",
        "print(classification_report(y_test, result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
            "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
            "       max_depth=5, min_child_weight=1, missing=None, n_estimators=50,\n",
            "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
            "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "       silent=True, subsample=1)\n",
            "XGBoost with GS\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.65      0.71      6499\n",
            "           1       0.98      0.96      0.97     14882\n",
            "           2       0.81      0.89      0.85     12360\n",
            "\n",
            "   micro avg       0.88      0.88      0.88     33741\n",
            "   macro avg       0.86      0.84      0.84     33741\n",
            "weighted avg       0.88      0.88      0.88     33741\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4UCCvMuWY8v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ПОСЫЛКА"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "koWBuiePnZt6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_send_corpus = transform(test['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "r44MiFDfneAw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_send = tfidfvectorizer.transform(X_test_send_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSTt7QoBWY9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_send = model.predict(X_test_send)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kl82T4SLWY9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('send.csv', 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i in range(len(result_send)):\n",
        "        f.write(str(i) + ',' + str(result_send[i]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O01TbJ90WY9Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ОБУЧИТЬ СВМ НА ВСЕХ ДАННЫХ"
      ]
    },
    {
      "metadata": {
        "id": "yRzpeJBQWY9Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, tfidfvectorizer = make_tfidf_vectorizer(transform(data['text']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2VbOj3nWY9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#one = SVCmod()\n",
        "#model = GradientBoostingClassifier(init=one, n_estimators=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OdR2WGJujy7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LinearSVC()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEXgMnECWY9s",
        "colab_type": "code",
        "outputId": "0e11d2a7-65b4-4f51-860d-fe483bb6b9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, data['label'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "nZ0ufuYFnGrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_send = tfidfvectorizer.transform(X_test_send_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b1T_uYGUWY95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_send = model.predict(X_test_send)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dJo6jKXAPOdy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('send.csv', 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i in range(len(result_send)):\n",
        "        f.write(str(i) + ',' + str(result_send[i]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tj9JhPbItXOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "АУГМЕНТАЦИЯ ТЕКСТА"
      ]
    },
    {
      "metadata": {
        "id": "pnJ-guyYE3Ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попытка аугментировать текст. Было нескольно вариантов: симметричная перестановка слов, рандомное удаление, и замена рандомного слова на синоним. Последнее кажется наиболее адекватным, но вроде бы русского тезауруса нормального нет и откуда брать синонимы, я не знаю. Поэтому осталось два предыдущих варианта. Я генерировал для каждого предложения рандомный 0 или 1. 0 для удаления рандомного слова, 1 для симметричной перестановки. После если 0, рандомное слово удаляется, и предложение с той же меткой добавляется в данные, если 1, то я выбирал два слова и переставлял их местами, потом добавлял это предложение в данные. Ну, результата это не дало, но я сейчас понимаю, что нужно было аугментировать не все классы, а наверно только тот, на котором классификатор больше всего ошибался или самый маленький класс"
      ]
    },
    {
      "metadata": {
        "id": "R6hxXzQmQyNQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################ augmentation function ########################\n",
        "\n",
        "def augmentation(text):\n",
        "  lists = []\n",
        "  for line in text:\n",
        "    lists.append(line.split())\n",
        "  lists_copy = lists.copy()\n",
        "  for sent in lists_copy:\n",
        "    mode = random.randint(0, 2)\n",
        "    length = len(sent)\n",
        "    sent_copy = sent.copy()\n",
        "    if not mode:\n",
        "      if len(sent)<=1:\n",
        "        lists.append(sent_copy)\n",
        "      else:\n",
        "        ind_1, ind_2 = random.choice(range(length), 2)\n",
        "        from_ = sent[ind_1]\n",
        "        sent_copy[ind_1] = sent_copy[ind_2]\n",
        "        sent_copy[ind_2] = from_\n",
        "        lists.append(sent_copy)\n",
        "    if mode:\n",
        "      if len(sent)<=1:\n",
        "        lists.append(sent_copy)\n",
        "      else:\n",
        "        delete = random.randint(low=0, high = length)\n",
        "        sent_copy.remove(sent_copy[delete])\n",
        "        lists.append(sent_copy)\n",
        "  texts = []\n",
        "  for line in lists:\n",
        "    texts.append(' '.join(line))\n",
        "  return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xb5uApXYwo0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############### augment the data ################\n",
        "\n",
        "text_corpus = transform(data['text'])\n",
        "text_aug = augmentation(text_corpus)\n",
        "X_train_aug, tfidf_aug = make_tfidf_vectorizer(text_aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpysvDWV3g2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################ get appropriate target #################\n",
        "target_aug = pd.concat((data['label'], data['label']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttDuemWqLOpk",
        "colab_type": "code",
        "outputId": "49b86156-2076-43e6-c5e6-ffa184aad2af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "################# fit the model ########################\n",
        "model  = LinearSVC()\n",
        "model.fit(X_train_aug, target_aug)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "sfvPBwETLyUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############## prepare test data ##############\n",
        "test_for_aug = tfidf_aug.transform(transform(test['text']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gcEbvdTOOkdc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############make predictions ################\n",
        "prediction_aug = model.predict(test_for_aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uFJD8ev3OwdE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## send solution #############\n",
        "with open('send.csv', 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i in range(len(prediction_aug)):\n",
        "        f.write(str(i) + ',' + str(prediction_aug[i]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}